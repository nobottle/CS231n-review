# CS231n Lec.4 Backpropagation and NN part 1

**backpropagation**

책에서 봤을 때 역전파라고 되어 있던 부분이다

gradient의 가중치를 업데이트 시키는 방법 중 하나이다

(x+y)z 가 있을 때 x=-2 y=5 z=-4일 경우

아래 식대로 했을 때

![스크린샷 2023-03-08 오후 2.20.17.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.20.17.png)

이렇게 초록색 부분들 값이 나온다

그걸 반대로 가는 과정(미분)을 알야아 하는데 df/dx,df/dy,df/dz라고 표현

q=x+y식에서  dq/dx는1, dq/dy=1이다

편미분의 과정이다

f=qz에서 df/dq는 z,df/dz=q이다

(원리는 아니까 생략하겟다)

여기서 알게되는 점은 덧셈연산에서의 미분은1이고 곱셈연산에서는 서로의 값을 가지게 된다는 것

![스크린샷 2023-03-08 오후 2.28.02.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.28.02.png)

빨간색 부분을 보면 역전파를 구한 과정이다 z,q까지는 쉬우니 생략하겟다

이제 y가 f x가 f에 미치는 영향 값을 구해보면

chain rule이 적용된다 >>미분에서 사용하는 방법인데 만약 x가 f에 대해서 영향을 미치는 값을 알고 싶으면

x>q>f이므로

x가 q에 미치는 영향*q가 f에 미치는 영향     을 구하면 된다.

위에서 q가 f에 미치는 영향은 예산을 해놧다 q=(x+y)인데 x+y의 미분은 1이다 >>이걸 local gradient라고 함

그러면 y가 q에 미치는 영향은?  y에대해 미분을 하면 되는데 값이1 므로 이거는 1이당

그래서1*q(-4)를 곱하면 -4가 된다

![스크린샷 2023-03-08 오후 2.39.54.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.39.54.png)

최종적으로 이렇게 식을 정리 할 수 있겠다

정리하자면 qz나 x+y같이 미리 구할 수 있는 미분 값을 local gradinet라고 함

이 local값과 앞에서 넘어온 gradient를 global gradient라고 하는데 이 값을 곱해서 gradient를 계산하는 식이다

![스크린샷 2023-03-08 오후 2.41.29.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.41.29.png)

정리하자면 이렇다!!

시그모이드 함수에 대해 봐보자

![스크린샷 2023-03-08 오후 2.52.49.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.52.49.png)

밑에 4개는 local gradient의 식이다

맨처음 gradient는 1

global gradientsms 1,그리고 1/x에서 미분하면 local gradient는 -1/x**2

x값을 대입하면 -0.53이 나오게 됨

그 다음은 더하기니까 그대로 흘러가고

exp시그모이드 함수가 나오는 부분

![스크린샷 2023-03-09 오후 5.07.46.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.07.46.png)

-0.53을 지수함수 미분값에 대입해서 대략 -0.2에 수렴하기 때문에 값은 -0.2가 나온다

![스크린샷 2023-03-09 오후 5.55.15.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.55.15.png)

덧셈노드는 그냥 그대로 흘러가고 곱하기노드는 반대편꺼 곱해서 계산하면 빨간색 역전파 값은 저렇게

나온다!!

근데 밑에 그림을 보면

![스크린샷 2023-03-09 오후 5.56.27.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.56.27.png)

저 부분은 시그모이드 함수 식이므로

시그모이드 도함수는y(1-y)의 값이므로 0.73을 대입해주면 0.2저렇게 한번에 나온다

그리고 위에서 덧셈노드는 그냥 그대로 흘러간다 그냥 gradient를 뿌려준다

그래서 이 덧셈노드는 distributor라고도 불린다

![스크린샷 2023-03-09 오후 5.58.39.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.58.39.png)

max gate란??

큰 값에게 gradient를 그대로 전하고 작은값은 0으로 만들어 전달

![스크린샷 2023-03-09 오후 6.00.05.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.00.05.png)

mul gate는 바꿔서 곱해준다 그래서 swither라고도 불림

그렇다면 이렇게 gradient가 한개가 아니라 여러개이면,,?

![스크린샷 2023-03-09 오후 6.01.26.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.01.26.png)

![스크린샷 2023-03-09 오후 6.07.45.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.07.45.png)

이렇게 이미지 데이터는 모든 픽셀 값들을 1차원 array로 재배치하기 때문에 차원이 굉장히 높다. class도 여러개인 이미지 데이터의 가중치는 엄청나게 큰 행렬일 수 밖에 없다. 이럴 경우에는 Jacobian matrix(행렬의 미분연산)를 이용한다는 설명 이 행렬은 너무 크지만 diagonal의 특성으로 대각행렬만 계산하면 되기 때문에 간단하다는 설명이다

![스크린샷 2023-03-09 오후 6.11.41.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.11.41.png)

그렇다면 행렬의 형태를 해보면 어떨까

L2의 최종값은 0.116, 출력노드의 미분값은 당연히 1

[0.22, 0.26]의 값은 W와 x의 행렬 연산으로 나온 값

![스크린샷 2023-03-09 오후 6.21.05.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.21.05.png)

f(q)로 함수를 지정했을때, 맨아래 식 처럼 q^2의 sum으로 표현 가능

이들은 전부 2q로 미분 되므로 가중치는 곱하기 2를 해준 [0.44, 0.52]가 된다

![스크린샷 2023-03-09 오후 6.22.38.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.22.38.png)

[0.2, 0.4] * [0.44, 0.52] 를 서로 행렬 계산해서 2*2 행렬을 만들어 준다 W가 2*2 행렬이기 때문

그래서 [[0.2*0.44, 0.4*0.44], [0.2*0.52, 0.4*0.52]]의 값이 도출된 것

![스크린샷 2023-03-09 오후 6.25.06.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.25.06.png)

T트랜스포즈를 한 이유는 행렬의 형태를(shape)를 맞춰야 하기 때문에

![스크린샷 2023-03-09 오후 6.30.41.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.30.41.png)

 이제 단순한 Linear regression이 아니라 히든레이어를 추가한다

히든 레이어를 input노드와 ouput노드 사이에 100개를 추가했고, 이를 102 - layer라고 하는게 아니라, **2 - layer NN이라고 한다 가중치w값이 있는 층만 층으로 세므로 input을 제외하면 2신경망 이렇게 된다**

![스크린샷 2023-03-09 오후 6.32.19.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.32.19.png)

이렇게 2-layer, 3-layer NN을 만드는데, 함수는 max함수를 이용하게 되낟

이는 활성화 함수를 말하는데 예전에는 시그모이드를 활성화 함수로 많이 이용했지만, 최근에는 relu함수를 많이 이용한다고 한다

![스크린샷 2023-03-09 오후 6.32.53.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.32.53.png)

활서화함수는 이렇게 여러개 있다 알아두도록 하장

![스크린샷 2023-03-09 오후 6.36.45.png](CS231n%20Lec%204%20Backpropagation%20and%20NN%20part%201%204414dc1e413a4d7283191591480f01d0/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-03-09_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_6.36.45.png)

이렇게 2-layer 나 3-layer은 fully하게 connected되어있다 줄여서 fc라고도 불림

(이렇게 인풋, 여러 개의 히든, 아웃풋 레이어는 전부 서로의 모든 노드에 관여하여 값을 도출한다)